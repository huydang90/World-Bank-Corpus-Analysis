---
title: "Code Reps 71-94 Test3"
author: "Thea Madsen"
date: "10/18/2019"
output:
  pdf_document:
    toc: yes
  html_document:
    number_sections: yes
    toc: yes
---
## Setup  
### Loading my packages  
```{r, echo = T, message=FALSE}
library(quanteda)
library(quanteda.corpora)
library(tm)
library(readtext)
library(ggplot2)
library(stringr)
```

### Setting up my directory  
```{r}
setwd("/Users/theamadsen/Desktop/Thea TAD MT Test")

path_data <- system.file("/Users/theamadsen/Desktop/Thea TAD MT Test", package = "readtext")
dat_thea_reps_71_94_test1 <- readtext(paste0(path_data, "Thea_Uncleaned_Reps_71-94*"))
                    docvarsfrom = "filenames" 
                    docvarnames = c("unit", "context", "year", "language")
                    dvsep = "_" 
                    encoding = "ISO-8859-1"

str(dat_thea_reps_71_94_test1)
```

### Making my corpus & summarizing it  
```{r}
doc.corpus <- corpus(dat_thea_reps_71_94_test1, text_field = "text")
summary(doc.corpus)
```

### Preparing my data: 
- Removing punctuation & digits

## Exploratory analysis  
1) Word cloud  
2) DMF  

### Cleaning and Creating Tokens  
```{r}
doc.tokens <- tokens(doc.corpus)

doc.tokens <- tokens(doc.tokens, remove_punct = TRUE, 
                     remove_numbers = TRUE)
doc.tokens <- tokens_select(doc.tokens, stopwords('english'),selection='remove')
doc.tokens <- tokens_wordstem(doc.tokens)
```

### Converting to DFM  
```{r}
doc.dfm <- dfm(doc.corpus, remove_numbers = TRUE, 
               stem = TRUE, 
               remove = stopwords("english"))

doc.dfm.final <- dfm(doc.tokens)
```

### Prelim Analysis  
```{r}
head(kwic(doc.tokens, "environment", window = 3))

topfeatures(doc.dfm.final, 5)

topfeatures(doc.dfm.final, 20)

textplot_wordcloud(doc.dfm.final)
```

### Lexical diversity and readability Test from Slava's code  
# KWIC

```{r}
head(kwic(doc.corpus, "develop"))

head(kwic(doc.corpus, "project"))
```

```{r}
textplot_xray(kwic(doc.corpus, pattern = "develop")) + 
    ggtitle("Lexical dispersion")
```

```{r}
textplot_xray(
    kwic(doc.corpus, pattern = "environment"),
    kwic(doc.corpus, pattern = "project")) + 
    ggtitle("Lexical dispersion")
```

# Collocations

Through collocation analysis, we can identify multi-word expressions that are very frequent in our corpus. 

```{r}
toks <- tokens(doc.corpus)
```

With `textstat_collocations()`, it is possible to discover multi-word expressions through statistical scoring of the associations of adjacent words.

One of the most common type of multi-word expressions is proper names, which we can select simply based on capitalization in English texts. If `textstat_collocations()` is applied to a tokens object comprised only of capitalize words, it usually returns multi-word proper names.

```{r}
col <- toks %>% 
       tokens_remove(stopwords("en")) %>% 
       tokens_select(pattern = "^[A-Z]", valuetype = "regex", 
                     case_insensitive = FALSE, padding = TRUE) %>% 
       textstat_collocations(min_count = 5, tolower = FALSE)
head(col)
```

Collocations are automatically recognized as multi-word expressions by `tokens_compound()` in case-sensitive fixed pattern matching. This is the fastest way to compound large numbers of multi-word expressions, but make sure that `tolower = FALSE` in `textstat_collocations()` to do this.

```{r}
comp_toks2 <- tokens_compound(toks, pattern = col)
head(kwic(comp_toks2, pattern = c("World_Bank", "Financial_Statements")))
```

You can use `phrase()` on collocations if more flexibility is needed. This is usually the case if you compound tokens from different corpus.

```{r}
comp_toks3 <- tokens_compound(toks, pattern = phrase(col$collocation))
head(kwic(comp_toks3, pattern = c("World_Bank", "Financial_Statements")))
```

The result of collocation analysis can be used to compound tokens. Compounding makes tokens less ambiguous and significantly improves quality of statistical analysis in the downstream. We will only compound strongly associated (p<0.005) multi-word expressions here by sub-setting `tstat_col_cap$collocation`.


```{r}
tstat_col_cap <- textstat_collocations(toks, min_count = 10, tolower = FALSE)
head(tstat_col_cap, 20)

```

### Readability measure World Bank Corpus Reports 1971-94


```{r}
# Readability measure setup World Bank Corpus reps 71-94
wb_read <- textstat_readability(doc.corpus,
                     measure = c("Flesch", "Flesch.Kincaid", "FOG"))
```

Notes FRE SCORES:

"Flesch reading ease measures how complex a text is. The lower the score, the more difficult the text is to read. The Flesch readability score uses the average length of your sentences (measured by the number of words) and the average number of syllables per word in an equation to calculate the reading ease. Text with a very high Flesch reading ease score (about 100) is straightforward and easy to read, with short sentences and no words of more than two syllables. The lower the Flesch reading score is, the harder it is to peruse the text."  


```{r}
# Ploting the distribution of FRE scores/years
plot(wb_read$Flesch, type = 'l', xaxt = 'n', xlab = "Flesch Reading Ease (FRE) Measure of World Bank Annual Reports", ylab = "Flesch Score")
grid()
axis(1, at = seq_len(nrow(wb_read)), labels = docvars(wb_corpus, 'Year'))
```


Notes FLESCH KINCAID SCORE:

"Flesch-Kincaid Grade Level classifications are based on:
- Sentence length as judged by the average number of words in a sentence
- Word length as judged by the average number of syllables in a word.
The higher the score is, the more difficult the text is."

```{r}
plot(wb_read$Flesch.Kincaid, type = 'l', xaxt = 'n', xlab = "Flesch Kincaid Measure of World Bank Annual Reports", ylab = "Flesch Kincaid Score")
grid()
axis(1, at = seq_len(nrow(wb_read)), labels = docvars(wb_corpus, 'Year'))
```

Notes GUNNING FOG:  

"The Gunning Fog formula generates a grade level, typically between 0 and 20. The formula estimates the years of formal education the reader requires to understand the text on first reading.

The formula for Gunning Fog is 0.4 [(words/sentences) + 100 (complex words/words)], where complex words are defined as those containing three or more syllables.

- Score of 15-20 would indicate academic-level papers
- Score of over 20 denotes that this is a complex text"

```{r}
plot(wb_read$FOG, type = 'l', xaxt = 'n', xlab = "Gunning Fog Measure of World Bank Annual Reports", ylab = "Gunning Fog Score")
grid()
axis(1, at = seq_len(nrow(wb_read)), labels = docvars(wb_corpus, 'Year'))
```