---
title: "Thea_Reps_71-94_Test1"
output:
  html_document:
    df_print: paged
---

### Data loading  
```{r}
setwd("/Users/theamadsen/Desktop/Thea TAD MT Test")

library(quanteda)
library(readtext)
require(quanteda)
require(quanteda.corpora)
require(readtext)
require(ggplot2)

path_data <- system.file("/Users/theamadsen/Desktop/Thea TAD MT Test", package = "readtext")
dat_thea_reps_71_94_test1 <- readtext(paste0(path_data, "Thea_Uncleaned_Reps_71-94*"))
                    docvarsfrom = "filenames" 
                    docvarnames = c("unit", "context", "year", "language")
                    dvsep = "_" 
                    encoding = "ISO-8859-1"

str(dat_thea_reps_71_94_test1)
```


### Making my corpus  
```{r}
doc.corpus <- corpus(dat_thea_reps_71_94_test1)
summary(doc.corpus)
```

Lexical diversity and readability Test from Slava's code
# KWIC

```{r}
head(kwic(doc.corpus, "develop"))

head(kwic(doc.corpus, "project"))
```

```{r}
textplot_xray(kwic(doc.corpus, pattern = "develop")) + 
    ggtitle("Lexical dispersion")
```



```{r}
textplot_xray(
    kwic(doc.corpus, pattern = "environment"),
    kwic(doc.corpus, pattern = "project")) + 
    ggtitle("Lexical dispersion")
```

# Collocations

Through collocation analysis, we can identify multi-word expressions that are very frequent in our corpus. 

```{r}
toks <- tokens(doc.corpus)
```

With `textstat_collocations()`, it is possible to discover multi-word expressions through statistical scoring of the associations of adjacent words.

One of the most common type of multi-word expressions is proper names, which we can select simply based on capitalization in English texts. If `textstat_collocations()` is applied to a tokens object comprised only of capitalize words, it usually returns multi-word proper names.

```{r}
col <- toks %>% 
       tokens_remove(stopwords("en")) %>% 
       tokens_select(pattern = "^[A-Z]", valuetype = "regex", 
                     case_insensitive = FALSE, padding = TRUE) %>% 
       textstat_collocations(min_count = 5, tolower = FALSE)
head(col)
```

Collocations are automatically recognized as multi-word expressions by `tokens_compound()` in case-sensitive fixed pattern matching. This is the fastest way to compound large numbers of multi-word expressions, but make sure that `tolower = FALSE` in `textstat_collocations()` to do this.

```{r}
comp_toks2 <- tokens_compound(toks, pattern = col)
head(kwic(comp_toks2, pattern = c("World_Bank", "Financial_Statements")))
```

You can use `phrase()` on collocations if more flexibility is needed. This is usually the case if you compound tokens from different corpus.

```{r}
comp_toks3 <- tokens_compound(toks, pattern = phrase(col$collocation))
head(kwic(comp_toks3, pattern = c("World_Bank", "Financial_Statements")))
```


The result of collocation analysis can be used to compound tokens. Compounding makes tokens less ambiguous and significantly improves quality of statistical analysis in the downstream. We will only compound strongly associated (p<0.005) multi-word expressions here by sub-setting `tstat_col_cap$collocation`.


```{r}
tstat_col_cap <- textstat_collocations(toks, min_count = 10, tolower = FALSE)
head(tstat_col_cap, 20)

```


```{r}
toks_comp <- tokens_compound(toks, pattern = tstat_col_cap[tstat_col_cap$z > 3])
toks[['2017-Trump']][1200:1350] # before compounding

```

```{r}
toks_comp[['2017-Trump']][1200:1350] # after compounding
```


Alternatively, wrap the whitespace-separated character vector by `phrase()` to compound multi-word expressions:

```{r}
toks_comp <- tokens_compound(toks, 
                             pattern =  phrase(tstat_col_cap$collocation[tstat_col_cap$z > 3]))
toks[['2013-Obama']][1200:1350] # before compounding

```


```{r}
toks_comp[['2013-Obama']][1200:1350] # after compounding
```


# Lexical diversity

`textstat_lexdiv()` calculates lexical diversity in various measures based on the number of unique types of tokens and the length of a document. It is useful for analysing speakers' or writers' linguistic skill, or complexity of ideas expressed in documents.


```{r}
dfmat_reps_71_94 <- dfm(doc.corpus, remove = stopwords('en'))
tstat_lexdiv <- textstat_lexdiv(dfmat_reps_71_94)
tail(tstat_lexdiv, 5)
```


```{r}
plot(tstat_lexdiv$TTR, type = 'l', xaxt = 'n', xlab = NULL, ylab = "TTR")
grid()
axis(1, at = seq_len(nrow(tstat_lexdiv)), labels = docvars(dfmat_reps_71_94, 'Titel'))
```

It can also calculate alternative metrics of lexical diversity:

```{r}
# variations of TTR
tstat_lexdiv <- textstat_lexdiv(dfmat_reps_71_94, measure=c("TTR", "R", "D"))
tail(tstat_lexdiv, 5)
# average-based methods
tstat_lexdiv_avg <- textstat_lexdiv(tokens(doc.corpus), measure="MATTR")
tail(tstat_lexdiv_avg, 5)

cor(cbind(tstat_lexdiv[,2:4], tstat_lexdiv_avg[,2]))
```

# Readability

`textstat_readability()` computes a metric of document complexity based on characteristics of the text such as number of words, sentence length, number of syllables, etc.

```{r}
sotu_read <- textstat_readability(doc.corpus, measure = "Flesch")
```


```{r}
stat_read <- textstat_readability(doc.corpus,
                     measure = c("Flesch.Kincaid", "FOG"))
plot(stat_read$Flesch.Kincaid, type = 'l', xaxt = 'n', xlab = NULL, ylab = "Flesch.Kincaid")
grid()
axis(1, at = seq_len(nrow(stat_read)), labels = docvars(dfmat_reps_71_94, 'President'))

````




















