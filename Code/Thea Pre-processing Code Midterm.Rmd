title: "Pre-processing World Bank Corpus"
author: "Thea Madsen"
output:
  pdf_document:
    toc: yes
  html_document:
    number_sections: yes
    toc: yes
---
    
```{r, echo = T, message=FALSE}
library(quanteda)
library(quanteda.corpora)
library(tm)
library(readtext)
library(ggplot2)
library(stringr)
```

```{r}
path_data <- setwd("/Users/theamadsen/Desktop/Thea TAD MT Test")
wb_corpus_files <- readtext(paste0(path_data, "/Uncleaned_Data/*"),
                    docvarsfrom = "filenames", 
                    dvsep = "_",
                    docvarnames = c("Organization", "Type", "Year"))
wb_corpus_files$doc_id <- str_replace(wb_corpus_files$doc_id , ".txt", "") %>%
   str_replace(. , "_\\d{2}", "")

# Create corpus object
wb_corpus <- corpus(wb_corpus_files, text_field = "text") 
summary(wb_corpus)
```

## Measure of readibility of World Bank Corpus  
```{r}
# Create readability measures for World Bank Corpus
wb_read <- textstat_readability(wb_corpus,
                     measure = c("Flesch", "Flesch.Kincaid", "FOG"))
```

### Plot distribution of Flesch Reading Ease (FRE) Measure scores over the years  
```{r}
plot(wb_read$Flesch, type = 'l', xaxt = 'n', xlab = "Flesch Reading Ease (FRE) Measure of World Bank Annual Reports", ylab = "Flesch Score")
grid()
axis(1, at = seq_len(nrow(wb_read)), labels = docvars(wb_corpus, 'Year'))
```

### Plot distribution of Flesch Kincaid Measure scores over the years  
```{r}
plot(wb_read$Flesch.Kincaid, type = 'l', xaxt = 'n', xlab = "Flesch Kincaid Measure of World Bank Annual Reports", ylab = "Flesch Kincaid Score")
grid()
axis(1, at = seq_len(nrow(wb_read)), labels = docvars(wb_corpus, 'Year'))
```

### Plot distribution of Gunning Fog Measure scores over the years  
```{r}
plot(wb_read$FOG, type = 'l', xaxt = 'n', xlab = "Gunning Fog Measure of World Bank Annual Reports", ylab = "Gunning Fog Score")
grid()
axis(1, at = seq_len(nrow(wb_read)), labels = docvars(wb_corpus, 'Year'))
```

### Creating Tokens & removing stopwords, punctuation and numbers  
```{r}
doc.tokens <- tokens(wb_corpus)

doc.tokens <- tokens(doc.tokens, remove_punct = TRUE, 
                     remove_numbers = TRUE)
doc.tokens <- tokens_select(doc.tokens, stopwords('english'),selection='remove')
doc.tokens <- tokens_wordstem(doc.tokens)
```

### Converting to DFM  
```{r}
doc.dfm <- dfm(wb_corpus, remove_numbers = TRUE, 
               stem = TRUE, 
               remove = stopwords("english"))

doc.dfm.final <- dfm(doc.tokens)
```

## Word cloud  
```{r}
head(kwic(doc.tokens, "environment", window = 3))

topfeatures(doc.dfm.final, 5)

topfeatures(doc.dfm.final, 20)

textplot_wordcloud(doc.dfm.final)
```

### Lexical diversity  
# KWIC  
**Thank you for code input assistance from our instructor Slava Jankin.**  
```{r}
head(kwic(wb_corpus, "develop"))

head(kwic(wb_corpus, "implemen"))
```

```{r}
textplot_xray(kwic(wb_corpus, pattern = "South-South")) + 
    ggtitle("Lexical dispersion")
```

```{r}
textplot_xray(
    kwic(wb_corpus, pattern = "cost"),
    kwic(wb_corpus, pattern = "project")) + 
    ggtitle("Lexical dispersion")
```

```{r}
toks <- tokens(wb_corpus)
```

```{r}
col <- toks %>% 
       tokens_remove(stopwords("en")) %>% 
       tokens_select(pattern = "^[A-Z]", valuetype = "regex", 
                     case_insensitive = FALSE, padding = TRUE) %>% 
       textstat_collocations(min_count = 5, tolower = FALSE)
head(col)
```

```{r}
comp_toks2 <- tokens_compound(toks, pattern = col)
head(kwic(comp_toks2, pattern = c("World_Bank", "Financial_Statements")))
```

```{r}
comp_toks3 <- tokens_compound(toks, pattern = phrase(col$collocation))
head(kwic(comp_toks3, pattern = c("World_Bank", "Financial_Statements")))
```


```{r}
tstat_col_cap <- textstat_collocations(toks, min_count = 10, tolower = FALSE)
head(tstat_col_cap, 20)

```