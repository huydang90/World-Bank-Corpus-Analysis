x <- with(presidential_debates_2012, readability(dialogue, list(person)))
ggplot(x)
head(presidential_debates_2012)
x <- with(presidential_debates_2012, readability(dialogue, list(person, time)))
ggplot(x)
head(presidential_debates_2012)
x <- with(presidential_debates_2012, readability(dialogue, list(person, time)))
plot(x)
install.packages("tibble")
install.packages("tibble")
install.packages("tibble")
install.packages("tibble")
typeof(presidential_debates_2012)
class(presidential_debates_2012)
head(presidential_debates_2012)
library("quanteda")
library("tm")
library("readtext")
library("stringr")
library("ggplot2")
library("syllable")
head(presidential_debates_2012)
x <- with(presidential_debates_2012, readability(dialogue, list(person, time)))
class(presidential_debates_2012)
typeof(presidential_debates_2012)
str(presidential_debates_2012)
str(wb_corpus_files)
head(presidential_debates_2012)
x <- with(presidential_debates_2012, readability(dialogue, person))
wb_readability <- with(wb_corpus_files, readability(text, Year, order.by.readability = FALSE))
plot(wb_readability)
library("quanteda")
library("tm")
library("readtext")
library("stringr")
library("ggplot2")
library("syllable")
#Load data
path_data <- setwd("/Users/dangngochuy/Desktop/Hertie/Hertie\ 3rd\ semester/Natural\ Language\ Processing/World-Bank-Corpus-Analysis/Code")
wb_corpus_files <- readtext(paste0(path_data, "/OCR_Data/*"),
docvarsfrom = "filenames",
dvsep = "_",
docvarnames = c("Organization", "Type", "Year"),
encoding = "ISO-8859-1")
wb_corpus_files$doc_id <- str_replace(wb_corpus_files$doc_id , ".txt", "") %>%
str_replace(. , "_\\d{2}", "")
#create corpus object
wb_corpus <- corpus(wb_corpus_files, text_field = "text")
# Inspect corpus to detect data issues
summary(wb_corpus)
# Create readability measures for World Bank Corpus
wb_read <- textstat_readability(wb_corpus,
measure = c("Flesch", "Flesch.Kincaid", "FOG"))
# Plot distribution of FRE scores over the years
plot(wb_read$Flesch, type = 'l', xaxt = 'n', xlab = "Flesch Reading Ease (FRE) Measure of World Bank Annual Reports", ylab = "Flesch Score")
grid()
axis(1, at = seq_len(nrow(wb_read)), labels = docvars(wb_corpus, 'Year'))
plot(wb_read$Flesch.Kincaid, type = 'l', xaxt = 'n', xlab = "Flesch Kincaid Measure of World Bank Annual Reports", ylab = "Flesch Kincaid Score")
grid()
axis(1, at = seq_len(nrow(wb_read)), labels = docvars(wb_corpus, 'Year'))
plot(wb_read$FOG, type = 'l', xaxt = 'n', xlab = "Gunning Fog Measure of World Bank Annual Reports", ylab = "Gunning Fog Score")
grid()
axis(1, at = seq_len(nrow(wb_read)), labels = docvars(wb_corpus, 'Year'))
#set up readability score functions
flesch_kincaid_ <- function(n.words, n.sents, n.sylls, ...){
(.39*(n.words/n.sents)) + (11.8*(n.sylls/n.words)) - 15.9
}
gunning_fog_ <- function(n.words, n.sents, n.complexes, ...){
.4*((n.words/n.sents) + (100*(n.complexes/n.words)))
}
coleman_liau_ <- function(n.words, n.sents, n.chars, ...) {
(0.0588 * ((100 * n.chars)/n.words)) - (0.296 * ((100 * n.sents)/n.words)) - 15.8
}
smog_ <- function(n.sents, n.polys) {
(1.043 * sqrt(n.polys * (30/n.sents))) + 3.1291
}
#automated_readability_index_  <- function(n.words, n.sents, n.chars){
#    4.71 * (n.chars/n.words) + 0.5 * (n.words/n.sents) - 21.43
#}
automated_readability_index_  <- function(n.words, n.sents, n.chars){
out <- 4.71 * (n.chars/n.words) + 0.5 * (n.words/n.sents) - 21.43
ceiling(ifelse(out < 1, 1, out)) - 1
}
SE <- function(x) sqrt(stats::var(x)/length(x))
digit_format <- function (x, digits = 1) {
if (is.null(digits))
digits <- 3
if (length(digits) > 1) {
digits <- digits[1]
warning("Using only digits[1]")
}
x <- round(as.numeric(x), digits)
if (digits > 0)
x <- sprintf(paste0("%.", digits, "f"), x)
out <- gsub("^0(?=\\.)|(?<=-)0", "", x, perl = TRUE)
out[out == "NA"] <- NA
out
}
#' Grade Level Readability
#' Calculate the Flesch Kincaid, Gunning Fog Index, Coleman Liau, SMOG,
#' Automated Readability Index and an average of the 5 readability scores.
readability <- function(x, grouping.var, order.by.readability = TRUE, group.names, ...){
n.sents <- n.words <- n.complexes <- n.polys <- n.chars <- Flesch_Kincaid <-
Gunning_Fog_Index <- Coleman_Liau <- SMOG <- Automated_Readability_Index <-
Average_Grade_Level <- n.sylls <- NULL
if(is.null(grouping.var)) {
G <- "all"
grouping <- rep("all", length(x))
} else {
if (isTRUE(grouping.var)) {
G <- "id"
grouping <- seq_along(x)
} else {
if (is.list(grouping.var) & length(grouping.var) > 1) {
m <- unlist(as.character(substitute(grouping.var))[-1])
G <- sapply(strsplit(m, "$", fixed=TRUE), function(x) {
x[length(x)]
}
)
grouping <- grouping.var
} else {
G <- as.character(substitute(grouping.var))
G <- G[length(G)]
grouping <- unlist(grouping.var)
}
}
}
if(!missing(group.names)) {
G <- group.names
}
y <- syllable::readability_word_stats_by(x, grouping, group.names = G)
grouping <- attributes(y)[["groups"]]
out <- y[, list(
Flesch_Kincaid = flesch_kincaid_(n.words, n.sents, n.sylls),
Gunning_Fog_Index = gunning_fog_(n.words, n.sents, n.complexes),
Coleman_Liau  =  coleman_liau_(n.words, n.sents, n.chars),
SMOG  =  smog_(n.sents, n.polys),
Automated_Readability_Index = automated_readability_index_(n.words, n.sents, n.chars)
), by = grouping][, list(
Average_Grade_Level = mean(c(Flesch_Kincaid, Gunning_Fog_Index, Coleman_Liau, SMOG, Automated_Readability_Index), na.rm=TRUE)
), by = c(
grouping, "Flesch_Kincaid", "Gunning_Fog_Index", "Coleman_Liau", "SMOG", "Automated_Readability_Index"
)]
if (isTRUE(order.by.readability)){
data.table::setorder(out, -Average_Grade_Level)
}
class(out) <- unique(c("readability", class(out)))
attributes(out)[["groups"]] <- grouping
out
}
#' Plots a readability Object
plot.readability <- function(x, ...){
Value <- NULL
x[["grouping.var"]] <- apply(x[, attributes(x)[["groups"]], with = FALSE], 1, paste, collapse = ".")
x[["grouping.var"]] <- factor(x[["grouping.var"]], levels = rev(x[["grouping.var"]]))
x <- x[, attributes(x)[["groups"]]:=NULL]
y <- tidyr::gather_(x, "Measure", "Value", c("Flesch_Kincaid", "Gunning_Fog_Index",
"Coleman_Liau", "SMOG", "Automated_Readability_Index"))
y[["Measure"]] <- gsub("_", " ", y[["Measure"]])
data.table::setDT(y)
center_dat <- y[, list(upper = mean(Value) + SE(Value), lower = mean(Value) - SE(Value),
means = mean(Value)), keyby = "grouping.var"]
nms <- gsub("(^|[[:space:]])([[:alpha:]])", "\\1\\U\\2", attributes(x)[["groups"]], perl=TRUE)
xaxis <- floor(min(y[["Value"]])):ceiling(max(y[["Value"]]))
ggplot2::ggplot(y, ggplot2::aes_string(y = "grouping.var")) +
ggplot2::geom_vline(xintercept = mean(center_dat[["means"]]), size=.75, alpha = .25, linetype="dashed") +
# ggplot2::geom_point(ggplot2::aes_string(color = "Measure", x = "Value"), size=1.4) +
ggplot2::geom_point(ggplot2::aes_string(color = "Measure", x = "Value"), size=2, shape=1) +
ggplot2::geom_errorbarh(data = center_dat, size=.75, alpha=.4,
ggplot2::aes_string(x = "means", xmin="upper", xmax="lower"), height = .3) +
ggplot2::geom_point(data = center_dat, ggplot2::aes_string(x = "means"), alpha = .5, shape=15, size=3) +
ggplot2::geom_point(data = center_dat, ggplot2::aes_string(x = "means"), size=1) +
ggplot2::scale_x_continuous(breaks = xaxis) +
ggplot2::ylab(paste(nms, collapse = " & ")) +
ggplot2::xlab("Grade Level") +
ggplot2::theme_bw() +
ggplot2::scale_color_discrete(name="Readability\nScore")
}
#' Prints a readability Object
print.readability <- function(x, digits = 1, ...){
key_id <- NULL
colord <-colnames(x)
cols <- c("Flesch_Kincaid", "Gunning_Fog_Index", "Coleman_Liau",
"SMOG", "Automated_Readability_Index", "Average_Grade_Level")
x[["key_id"]] <- 1:nrow(x)
y <- tidyr::gather_(x, "measure", "value", cols)
y[["value"]] <- digit_format(y[["value"]], digits)
y <- tidyr::spread_(y, "measure", "value")
data.table::setDT(y)
y <- y[order(key_id)]
y[, "key_id"] <- NULL
data.table::setcolorder(y, colord)
print(y)
}
wb_readability <- with(wb_corpus_files, readability(text, Year, order.by.readability = FALSE))
plot(wb_readability)
library("quanteda")
library("tm")
library("readtext")
library("stringr")
library("ggplot2")
library("syllable")
#Load data
path_data <- setwd("/Users/dangngochuy/Desktop/Hertie/Hertie\ 3rd\ semester/Natural\ Language\ Processing/World-Bank-Corpus-Analysis/Code")
wb_corpus_files <- readtext(paste0(path_data, "/OCR_Data/*"),
docvarsfrom = "filenames",
dvsep = "_",
docvarnames = c("Organization", "Type", "Year"),
encoding = "ISO-8859-1")
wb_corpus_files$doc_id <- str_replace(wb_corpus_files$doc_id , ".txt", "") %>%
str_replace(. , "_\\d{2}", "")
#create corpus object
wb_corpus <- corpus(wb_corpus_files, text_field = "text")
# Inspect corpus to detect data issues
summary(wb_corpus)
# Create readability measures for World Bank Corpus
wb_read <- textstat_readability(wb_corpus,
measure = c("Flesch", "Flesch.Kincaid", "FOG"))
# Plot distribution of FRE scores over the years
plot(wb_read$Flesch, type = 'l', xaxt = 'n', xlab = "Flesch Reading Ease (FRE) Measure of World Bank Annual Reports", ylab = "Flesch Score")
grid()
axis(1, at = seq_len(nrow(wb_read)), labels = docvars(wb_corpus, 'Year'))
#Load data
path_data <- setwd("/Users/dangngochuy/Desktop/Hertie/Hertie\ 3rd\ semester/Natural\ Language\ Processing/World-Bank-Corpus-Analysis/Code")
wb_corpus_files <- readtext(paste0(path_data, "/OCR_Data/*"),
docvarsfrom = "filenames",
dvsep = "_",
docvarnames = c("Organization", "Type", "Year"),
encoding = "ISO-8859-1")
wb_corpus_files$doc_id <- str_replace(wb_corpus_files$doc_id , ".txt", "") %>%
str_replace(. , "_\\d{2}", "")
#create corpus object
wb_corpus <- corpus(wb_corpus_files, text_field = "text")
# Inspect corpus to detect data issues
summary(wb_corpus)
library("quanteda")
library("tm")
library("readtext")
library("stringr")
library("ggplot2")
library("syllable")
#Load data
path_data <- setwd("/Users/dangngochuy/Desktop/Hertie/Hertie\ 3rd\ semester/Natural\ Language\ Processing/World-Bank-Corpus-Analysis/Code")
wb_corpus_files <- readtext(paste0(path_data, "/OCR_Data/*"),
docvarsfrom = "filenames",
dvsep = "_",
docvarnames = c("Organization", "Type", "Year"),
encoding = "ISO-8859-1")
wb_corpus_files$doc_id <- str_replace(wb_corpus_files$doc_id , ".txt", "") %>%
str_replace(. , "_\\d{2}", "")
#create corpus object
wb_corpus <- corpus(wb_corpus_files, text_field = "text")
# Inspect corpus to detect data issues
summary(wb_corpus)
# Create readability measures for World Bank Corpus
wb_read <- textstat_readability(wb_corpus,
measure = c("Flesch", "Flesch.Kincaid", "FOG"))
# Plot distribution of FRE scores over the years
# Start the clock!
ptm <- proc.time()
plot(wb_read$Flesch, type = 'l', xaxt = 'n', xlab = "Flesch Reading Ease (FRE) Measure of World Bank Annual Reports", ylab = "Flesch Score")
grid()
axis(1, at = seq_len(nrow(wb_read)), labels = docvars(wb_corpus, 'Year'))
# Stop the clock
proc.time() - ptm
# Plot distribution of FRE scores over the years
plot(wb_read$Flesch, type = 'l', xaxt = 'n', xlab = "Flesch Reading Ease (FRE) Measure of World Bank Annual Reports", ylab = "Flesch Score")
grid()
axis(1, at = seq_len(nrow(wb_read)), labels = docvars(wb_corpus, 'Year'))
plot(wb_read$Flesch.Kincaid, type = 'l', xaxt = 'n', xlab = "Flesch Kincaid Measure of World Bank Annual Reports", ylab = "Flesch Kincaid Score")
grid()
axis(1, at = seq_len(nrow(wb_read)), labels = docvars(wb_corpus, 'Year'))
plot(wb_read$FOG, type = 'l', xaxt = 'n', xlab = "Gunning Fog Measure of World Bank Annual Reports", ylab = "Gunning Fog Score")
grid()
axis(1, at = seq_len(nrow(wb_read)), labels = docvars(wb_corpus, 'Year'))
#set up readability score functions
flesch_kincaid_ <- function(n.words, n.sents, n.sylls, ...){
(.39*(n.words/n.sents)) + (11.8*(n.sylls/n.words)) - 15.9
}
gunning_fog_ <- function(n.words, n.sents, n.complexes, ...){
.4*((n.words/n.sents) + (100*(n.complexes/n.words)))
}
coleman_liau_ <- function(n.words, n.sents, n.chars, ...) {
(0.0588 * ((100 * n.chars)/n.words)) - (0.296 * ((100 * n.sents)/n.words)) - 15.8
}
smog_ <- function(n.sents, n.polys) {
(1.043 * sqrt(n.polys * (30/n.sents))) + 3.1291
}
#automated_readability_index_  <- function(n.words, n.sents, n.chars){
#    4.71 * (n.chars/n.words) + 0.5 * (n.words/n.sents) - 21.43
#}
automated_readability_index_  <- function(n.words, n.sents, n.chars){
out <- 4.71 * (n.chars/n.words) + 0.5 * (n.words/n.sents) - 21.43
ceiling(ifelse(out < 1, 1, out)) - 1
}
SE <- function(x) sqrt(stats::var(x)/length(x))
digit_format <- function (x, digits = 1) {
if (is.null(digits))
digits <- 3
if (length(digits) > 1) {
digits <- digits[1]
warning("Using only digits[1]")
}
x <- round(as.numeric(x), digits)
if (digits > 0)
x <- sprintf(paste0("%.", digits, "f"), x)
out <- gsub("^0(?=\\.)|(?<=-)0", "", x, perl = TRUE)
out[out == "NA"] <- NA
out
}
#' Grade Level Readability
#' Calculate the Flesch Kincaid, Gunning Fog Index, Coleman Liau, SMOG,
#' Automated Readability Index and an average of the 5 readability scores.
readability <- function(x, grouping.var, order.by.readability = TRUE, group.names, ...){
n.sents <- n.words <- n.complexes <- n.polys <- n.chars <- Flesch_Kincaid <-
Gunning_Fog_Index <- Coleman_Liau <- SMOG <- Automated_Readability_Index <-
Average_Grade_Level <- n.sylls <- NULL
if(is.null(grouping.var)) {
G <- "all"
grouping <- rep("all", length(x))
} else {
if (isTRUE(grouping.var)) {
G <- "id"
grouping <- seq_along(x)
} else {
if (is.list(grouping.var) & length(grouping.var) > 1) {
m <- unlist(as.character(substitute(grouping.var))[-1])
G <- sapply(strsplit(m, "$", fixed=TRUE), function(x) {
x[length(x)]
}
)
grouping <- grouping.var
} else {
G <- as.character(substitute(grouping.var))
G <- G[length(G)]
grouping <- unlist(grouping.var)
}
}
}
if(!missing(group.names)) {
G <- group.names
}
y <- syllable::readability_word_stats_by(x, grouping, group.names = G)
grouping <- attributes(y)[["groups"]]
out <- y[, list(
Flesch_Kincaid = flesch_kincaid_(n.words, n.sents, n.sylls),
Gunning_Fog_Index = gunning_fog_(n.words, n.sents, n.complexes),
Coleman_Liau  =  coleman_liau_(n.words, n.sents, n.chars),
SMOG  =  smog_(n.sents, n.polys),
Automated_Readability_Index = automated_readability_index_(n.words, n.sents, n.chars)
), by = grouping][, list(
Average_Grade_Level = mean(c(Flesch_Kincaid, Gunning_Fog_Index, Coleman_Liau, SMOG, Automated_Readability_Index), na.rm=TRUE)
), by = c(
grouping, "Flesch_Kincaid", "Gunning_Fog_Index", "Coleman_Liau", "SMOG", "Automated_Readability_Index"
)]
if (isTRUE(order.by.readability)){
data.table::setorder(out, -Average_Grade_Level)
}
class(out) <- unique(c("readability", class(out)))
attributes(out)[["groups"]] <- grouping
out
}
#' Plots a readability Object
plot.readability <- function(x, ...){
Value <- NULL
x[["grouping.var"]] <- apply(x[, attributes(x)[["groups"]], with = FALSE], 1, paste, collapse = ".")
x[["grouping.var"]] <- factor(x[["grouping.var"]], levels = rev(x[["grouping.var"]]))
x <- x[, attributes(x)[["groups"]]:=NULL]
y <- tidyr::gather_(x, "Measure", "Value", c("Flesch_Kincaid", "Gunning_Fog_Index",
"Coleman_Liau", "SMOG", "Automated_Readability_Index"))
y[["Measure"]] <- gsub("_", " ", y[["Measure"]])
data.table::setDT(y)
center_dat <- y[, list(upper = mean(Value) + SE(Value), lower = mean(Value) - SE(Value),
means = mean(Value)), keyby = "grouping.var"]
nms <- gsub("(^|[[:space:]])([[:alpha:]])", "\\1\\U\\2", attributes(x)[["groups"]], perl=TRUE)
xaxis <- floor(min(y[["Value"]])):ceiling(max(y[["Value"]]))
ggplot2::ggplot(y, ggplot2::aes_string(y = "grouping.var")) +
ggplot2::geom_vline(xintercept = mean(center_dat[["means"]]), size=.75, alpha = .25, linetype="dashed") +
# ggplot2::geom_point(ggplot2::aes_string(color = "Measure", x = "Value"), size=1.4) +
ggplot2::geom_point(ggplot2::aes_string(color = "Measure", x = "Value"), size=2, shape=1) +
ggplot2::geom_errorbarh(data = center_dat, size=.75, alpha=.4,
ggplot2::aes_string(x = "means", xmin="upper", xmax="lower"), height = .3) +
ggplot2::geom_point(data = center_dat, ggplot2::aes_string(x = "means"), alpha = .5, shape=15, size=3) +
ggplot2::geom_point(data = center_dat, ggplot2::aes_string(x = "means"), size=1) +
ggplot2::scale_x_continuous(breaks = xaxis) +
ggplot2::ylab(paste(nms, collapse = " & ")) +
ggplot2::xlab("Grade Level") +
ggplot2::theme_bw() +
ggplot2::scale_color_discrete(name="Readability\nScore")
}
#' Prints a readability Object
print.readability <- function(x, digits = 1, ...){
key_id <- NULL
colord <-colnames(x)
cols <- c("Flesch_Kincaid", "Gunning_Fog_Index", "Coleman_Liau",
"SMOG", "Automated_Readability_Index", "Average_Grade_Level")
x[["key_id"]] <- 1:nrow(x)
y <- tidyr::gather_(x, "measure", "value", cols)
y[["value"]] <- digit_format(y[["value"]], digits)
y <- tidyr::spread_(y, "measure", "value")
data.table::setDT(y)
y <- y[order(key_id)]
y[, "key_id"] <- NULL
data.table::setcolorder(y, colord)
print(y)
}
# Start the clock!
ptm <- proc.time()
wb_readability <- with(wb_corpus_files, readability(text, Year, order.by.readability = FALSE))
# Stop the clock
proc.time() - ptm
plot(wb_readability)
head(wb_read)
str(wb_read)
ggplot(wb_read, aes(x = Year, y = Flesch)) +
geom_point(color = "firebrick") +
labs(x = "Year", y = "Flesch Score") +
ggtitle("Flesch Reading Ease (FRE) Measure of World Bank Annual Reports 1947-2018") +
theme(plot.title = element_text(size = 16, face = "bold",
vjust = 1, lineheight = 0.75))
ggplot(wb_read, aes(x = wb_corpus$Year, y = Flesch)) +
geom_point(color = "firebrick") +
labs(x = "Year", y = "Flesch Score") +
ggtitle("Flesch Reading Ease (FRE) Measure of World Bank Annual Reports 1947-2018") +
theme(plot.title = element_text(size = 16, face = "bold",
vjust = 1, lineheight = 0.75))
ggplot(aes(x = wb_corpus$Year, y = wb_read$Flesch)) +
geom_point(color = "firebrick") +
labs(x = "Year", y = "Flesch Score") +
ggtitle("Flesch Reading Ease (FRE) Measure of World Bank Annual Reports 1947-2018") +
theme(plot.title = element_text(size = 16, face = "bold",
vjust = 1, lineheight = 0.75))
library("quanteda")
library("tm")
library("readtext")
library("stringr")
library("ggplot2")
library("syllable")
install.packages("stm")
library("quanteda")
library("tm")
library("readtext")
library("stringr")
library("ggplot2")
library("syllable")
library("stm")
#Load data
path_data <- setwd("/Users/dangngochuy/Desktop/Hertie/Hertie\ 3rd\ semester/Natural\ Language\ Processing/World-Bank-Corpus-Analysis/Code")
wb_corpus_files <- readtext(paste0(path_data, "/OCR_Data/*"),
docvarsfrom = "filenames",
dvsep = "_",
docvarnames = c("Organization", "Type", "Year"),
encoding = "ISO-8859-1")
wb_corpus_files$doc_id <- str_replace(wb_corpus_files$doc_id , ".txt", "") %>%
str_replace(. , "_\\d{2}", "")
setwd("~/Desktop/Hertie/Hertie 3rd semester/Natural Language Processing")
#Load data
path_data <- setwd("/Users/dangngochuy/Desktop/Hertie/Hertie\ 3rd\ semester/Natural\ Language\ Processing/World-Bank-Corpus-Analysis/Code")
wb_corpus_files <- readtext(paste0(path_data, "/OCR_Data/*"),
docvarsfrom = "filenames",
dvsep = "_",
docvarnames = c("Organization", "Type", "Year"),
encoding = "ISO-8859-1")
wb_corpus_files$doc_id <- str_replace(wb_corpus_files$doc_id , ".txt", "") %>%
str_replace(. , "_\\d{2}", "")
setwd("~/Desktop/Hertie/Hertie 3rd semester/Natural Language Processing/Code")
#Load data
path_data <- setwd("/Users/dangngochuy/Desktop/Hertie/Hertie\ 3rd\ semester/Natural\ Language\ Processing/World-Bank-Corpus-Analysis/Code")
wb_corpus_files <- readtext(paste0(path_data, "/OCR_Data/*"),
docvarsfrom = "filenames",
dvsep = "_",
docvarnames = c("Organization", "Type", "Year"),
encoding = "ISO-8859-1")
wb_corpus_files$doc_id <- str_replace(wb_corpus_files$doc_id , ".txt", "") %>%
str_replace(. , "_\\d{2}", "")
#create corpus object
wb_corpus <- corpus(wb_corpus_files, text_field = "text")
install.packages("igraph")
install.packages("stmCorrViz")
library("quanteda")
library("tm")
library("readtext")
library("stringr")
library("ggplot2")
library("syllable")
library("stm")
library("stmCorrViz")
library("igraph")
# Inspect corpus to detect data issues
summary(wb_corpus)
View(wb_corpus_files)
processed <- textProcessor(wb_corpus_files$text, metadata=data)
out <- prepDocuments(processed$documents, processed$vocab, processed$meta)
processed <- textProcessor(wb_corpus_files$text, metadata=wb_corpus_files)
View(wb_corpus_files)
out <- prepDocuments(processed$documents, processed$vocab, processed$meta)
docs <- out$documents
vocab <- out$vocab
meta <- out$meta
plotRemoved(processed$documents, lower.thresh=seq(1,200, by=100))
#Next, estimate the structural topic model with the topic prevalence parameter. To do this, execute an stm model using the 'out' data with 20 topics. Here we can ask how prevalence of topics varies across documents' meta data, including 'rating' and 'day'. The option 's(day)' applies a spline normalization to 'day' variable. The stm R package authors specified the maximum number of expectation-maximization iterations = 75, and the seed they used for reproducibility.
poliblogPrevFit <- stm(out$documents, out$vocab, K=20, prevalence=~Year,
max.em.its=75, data=out$meta, init.type="Spectral",
seed=8458159)
#summary model with 20 topics
plot(poliblogPrevFit, type="summary", xlim=c(0,.4))
plot(poliblogPrevFit, type="hist")
poliblogSelect <- selectModel(out$documents, out$vocab, K=20, prevalence=~Year,
max.em.its=75, data=meta, runs=20, seed=8458159)
topicQuality(model=poliblogPrevFit, documents=docs)
mod.out.corr <- topicCorr(poliblogPrevFit)
plot(mod.out.corr)
